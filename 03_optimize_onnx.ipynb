{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321fde30-f658-4963-b78c-d4ef9dc0dd19",
   "metadata": {},
   "source": [
    "# End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization\n",
    "From https://github.com/huggingface/blog/blob/main/optimum-inference.md\n",
    "\n",
    "In this End-to-End tutorial on accelerating RoBERTa for question-answering, you will learn how to:\n",
    "\n",
    "1. Install Optimum for ONNX Runtime\n",
    "2. Convert a Hugging Face Transformers model to ONNX for inference\n",
    "3. Use the ORTOptimizer to optimize the model\n",
    "4. Use the ORTQuantizer to apply dynamic quantization\n",
    "5. Run accelerated inference using Transformers pipelines\n",
    "6. Evaluate the performance and speed\n",
    "   \n",
    "Letâ€™s get started ðŸš€\n",
    "\n",
    "This tutorial was created and run on an m5.xlarge AWS EC2 Instance and also works on the SUTD cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dced95",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab790e",
   "metadata": {},
   "source": [
    "###  1. Transformer Model\n",
    "###  2. ONNX Model\n",
    "### 3. Optimized ONNX Model\n",
    "### 4. Quantized Optimized ONNX Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c2b473",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc654fc0-a16d-4571-b3f8-b2e41e2454d6",
   "metadata": {},
   "source": [
    "## Install Optimum for Onnxruntime\n",
    "Our first step is to install Optimum with the onnxruntime utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e0c5f12-0313-4632-b27f-01dd8ca36466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install optimum[onnxruntime]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce170e-a692-45ef-b665-1543a9ef212a",
   "metadata": {},
   "source": [
    "## 3.2 Convert a Hugging Face Transformers model to ONNX for inference\n",
    "Before we can start optimizing we need to convert our vanilla transformers model to the onnx format. The model we are using is the deepset/roberta-base-squad2 a fine-tuned RoBERTa model on the SQUAD2 question answering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64858a5-0e67-49db-9c5e-899c248e9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.904166042804718, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path # Library for os-independent file paths\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\" # A fine-tuned RoBERTa model trained for question-answering on the SQuAD2 dataset\n",
    "onnx_path = Path(\"onnx\") # Path to save the onnx model\n",
    "task = \"question-answering\" # The task for the pipeline\n",
    "\n",
    "\n",
    "# Loads the original RoBERTa model from Hugging Face and automatically converts it to ONNX\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "# Loads the pre-trained tokenizer for RoBERTa.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "# Saves the ONNX-optimized version of the model\n",
    "model.save_pretrained(onnx_path)\n",
    "# Saves the tokenizer files\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "\n",
    "# Creates a Hugging Face pipeline for question-answering\n",
    "# Uses the optimized ONNX model\n",
    "optimum_qa = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "\n",
    "\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706078b-b646-4cdf-8677-1323545a0f2c",
   "metadata": {},
   "source": [
    "We successfully converted our vanilla transformers to onnx and used the model with the transformers.pipelines to run the first prediction. Now let's optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3daaa-bb98-4917-9326-18ba2b10266d",
   "metadata": {},
   "source": [
    "## Use the ORTOptimizer to optimize the model\n",
    "After we saved our onnx checkpoint to onnx/ we can now use the ORTOptimizer to apply graph optimization, such as operator fusion and constant folding to accelerate latency and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9ec7ea",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "1. Constant Folding: Precomputes constant expressions during model conversion.\n",
    "For example: If the model has 3 + 5, it replaces it with 8, reducing runtime computation.\n",
    "\n",
    "2. Operator Fusion: Merges multiple operations into one to reduce computation overhead.\n",
    "Example: Instead of MatMul â†’ Add â†’ Activation, it creates a single FusedLayer.\n",
    "\n",
    "3. Graph Pruning: Removes unused or redundant nodes to make inference faster.\n",
    "\n",
    "4. Memory Optimization: Rearranges tensor allocations to reduce memory usage.\n",
    "\n",
    "5. Hardware-Specific Optimizations: Uses CPU/GPU-specific instructions (e.g., AVX, Tensor Cores) for speedup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e3ae36",
   "metadata": {},
   "source": [
    "### Why Isn't Optimization Applied from the Beginning?\n",
    "\n",
    "The main reasons involve trade-offs between flexibility, generalization, hardware compatibility, and debugging ease.\n",
    "\n",
    "for example:\n",
    "- Inference only needs forward passes, so operations can be fused.\n",
    "- Unoptimized models keep each operation separate and readable, which makes debugging easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafe50d-a582-4e25-8ff1-b80b32796274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0;93m2025-02-20 14:07:47.393799 [W:onnxruntime:, inference_session.cc:2048 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "\n",
    "# Loads the previously saved ONNX model from the onnx/ directory.\n",
    "optimizer = ORTOptimizer.from_pretrained(onnx_path)\n",
    "\n",
    "\n",
    "# Defines how aggressively the model should be optimized.\n",
    "# The optimization level 99 applies all possible optimizations.\n",
    "optimization_config = OptimizationConfig(optimization_level=99)\n",
    "\n",
    "\n",
    "# Apply the optimization configuration to the model\n",
    "optimizer.optimize(save_dir=onnx_path, optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb2961-1b15-4356-aa80-a0eb63c249cc",
   "metadata": {},
   "source": [
    "To test performance we can use the ORTModelForQuestionAnswering class again and provide an additional file_name parameter to load our optimized model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81fd1c42-7921-4b54-b037-1b9974230772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041661620140076, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "# load optimized model\n",
    "opt_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "opt_optimum_qa = pipeline(task, model=opt_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = opt_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "print(prediction)\n",
    "# {'score': 0.9041661620140076, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc696-72b2-45af-9b2f-310ca4d70e56",
   "metadata": {},
   "source": [
    "## Use the ORTQuantizer to apply dynamic quantization\n",
    "Another option to reduce model size and accelerate inference is by quantizing the model using the ORTQuantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6470da15-abf3-4b45-9eee-65a8bc42de76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "\n",
    "# Load the ONNX Model for Quantization\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_path, file_name=\"model.onnx\")\n",
    "# Define the Quantization Configuration\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "\n",
    "# Quantize it!\n",
    "quantizer.quantize(save_dir=onnx_path, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784792ee-46d7-47f6-b995-3ce99fdb9d35",
   "metadata": {},
   "source": [
    "We can now compare this model size as well as some latency performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9af92e2c-a508-4f56-9f2a-adb2a5419706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Onnx Model file size: 473.54 MB\n",
      "Quantized Onnx Model file size: 119.61 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")\n",
    "\n",
    "# Vanilla Onnx Model file size: 473.51 MB\n",
    "# Quantized Onnx Model file size: 119.15 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c57cd0-2c1e-4972-8348-1a50cde5dea2",
   "metadata": {},
   "source": [
    "## Run accelerated inference using pipelines\n",
    "\n",
    "Optimum has built-in support for transformers pipelines. This allows us to leverage the same API that we know from using PyTorch and TensorFlow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f35cde70-69eb-418a-b3e7-e79d298c618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8086288571357727, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "quant_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "\n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=quant_model, tokenizer=tokenizer)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.806605339050293, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ab4bb-bb57-490e-862a-727d9fbe4b4c",
   "metadata": {},
   "source": [
    "In addition to this optimum has a pipelines API which guarantees more safety for your accelerated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "895cf453-8153-44ad-9fb2-3790482f6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8086288571357727, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "quant_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "                                                     \n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=quant_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.806605339050293, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb03c5-74cd-42f7-bcfe-02b53c5a095d",
   "metadata": {},
   "source": [
    "## Evaluate performance and speed\n",
    "As the last step, we want to take a detailed look at the performance and accuracy of our model. Applying optimization techniques, like graph optimizations or quantization not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.\n",
    "\n",
    "Let's evaluate our models. Our transformers model deepset/roberta-base-squad2 was fine-tuned on the SQUAD2 dataset. This will be the dataset we use to evaluate our models.\n",
    "To safe time, we only load 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6508dc2c-9d64-4193-89c8-59aff4381329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset 1187\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "import evaluate\n",
    "metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "# load 10% of the data to safe time\n",
    "# metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\", split=\"validation[:10%]\")\n",
    "\n",
    "print(f\"length of dataset {len(dataset)}\")\n",
    "#length of dataset 1187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4df13f1-b2ef-4db8-86cc-fe2fcc93c68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function evaluate at 0x44fe0beb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function evaluate at 0x44fe0beb0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1187/1187 [05:19<00:00,  3.72 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def evaluate(example):\n",
    "  default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  optimized = opt_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'optimized': {'id': example['id'],'prediction_text': optimized['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "result = dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea9364-d017-4cde-a394-6d6846c8e32e",
   "metadata": {},
   "source": [
    "Now lets compare the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c22246ce-d719-4323-b5c8-5678064a1bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla model: exact=81.12889637742207% f1=83.27089343306695%\n",
      "optimized model: exact=80.45492839090143% f1=83.27089343306695%\n",
      "quantized model: exact=80.45492839090143% f1=82.63289411141132%\n"
     ]
    }
   ],
   "source": [
    "default_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "optimized = metric.compute(predictions=result[\"optimized\"], references=result[\"reference\"])\n",
    "quantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])\n",
    "\n",
    "print(f\"vanilla model: exact={default_acc['exact']}% f1={default_acc['f1']}%\")\n",
    "print(f\"optimized model: exact={quantized['exact']}% f1={optimized['f1']}%\")\n",
    "print(f\"quantized model: exact={quantized['exact']}% f1={quantized['f1']}%\")\n",
    "\n",
    "# vanilla model: exact=81.12889637742207% f1=83.27089343306695%\n",
    "# quantized model: exact=80.6234203875316% f1=82.86541222514259%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a2b7b-cc99-46b1-8c2a-56a804b2c37f",
   "metadata": {},
   "source": [
    "The quantized model achived an exact match of 80.62% and an f1 score of 82.86% which is 99% of the original model.\n",
    "\n",
    "Okay, let's test the performance (latency) of our optimized and quantized model.\n",
    "\n",
    "But first, letâ€™s extend our context and question to a more meaningful sequence length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d30f32a3-65c5-4e0b-b72f-fc1b52a8948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\"\n",
    "question=\"As what is Philipp working?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec406102-ba65-4d05-bb10-10703753be58",
   "metadata": {},
   "source": [
    "To keep it simple, we are going to use a python loop and calculate the avg/mean latency for our vanilla model and for the optimized and quantized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ef28ced-87a0-4096-9a43-2dcdf3f6a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla model Average latency (ms) - 86.95 +\\- 7.75\n",
      "Optimized model Average latency (ms) - 85.69 +\\- 7.62\n",
      "Quantized model Average latency (ms) - 70.74 +\\- 2.71\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\"\n",
    "\n",
    "print(f\"Vanilla model {measure_latency(optimum_qa)}\")\n",
    "print(f\"Optimized model {measure_latency(opt_optimum_qa)}\")\n",
    "print(f\"Quantized model {measure_latency(quantized_optimum_qa)}\")\n",
    "\n",
    "# Vanilla model Average latency (ms) 102\n",
    "# Optimized model Average latency (ms) 101\n",
    "# Quantized model Average latency (ms) 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245313e-f7ea-427b-8fd5-d7ea3efdab68",
   "metadata": {},
   "source": [
    "We managed to reduce our model latency from 102ms to 46ms or by 55%, while keeping 99% of the accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
