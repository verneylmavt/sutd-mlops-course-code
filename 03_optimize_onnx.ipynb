{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "321fde30-f658-4963-b78c-d4ef9dc0dd19",
   "metadata": {},
   "source": [
    "# End-to-End tutorial on accelerating RoBERTa for Question-Answering including quantization and optimization\n",
    "From https://github.com/huggingface/blog/blob/main/optimum-inference.md\n",
    "\n",
    "In this End-to-End tutorial on accelerating RoBERTa for question-answering, you will learn how to:\n",
    "\n",
    "1. Install Optimum for ONNX Runtime\n",
    "2. Convert a Hugging Face Transformers model to ONNX for inference\n",
    "3. Use the ORTOptimizer to optimize the model\n",
    "4. Use the ORTQuantizer to apply dynamic quantization\n",
    "5. Run accelerated inference using Transformers pipelines\n",
    "6. Evaluate the performance and speed\n",
    "   \n",
    "Letâ€™s get started ðŸš€\n",
    "\n",
    "This tutorial was created and run on an m5.xlarge AWS EC2 Instance and also works on the SUTD cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc654fc0-a16d-4571-b3f8-b2e41e2454d6",
   "metadata": {},
   "source": [
    "## Install Optimum for Onnxruntime\n",
    "Our first step is to install Optimum with the onnxruntime utilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e0c5f12-0313-4632-b27f-01dd8ca36466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no matches found: optimum[onnxruntime]\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[onnxruntime]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce170e-a692-45ef-b665-1543a9ef212a",
   "metadata": {},
   "source": [
    "## 3.2 Convert a Hugging Face Transformers model to ONNX for inference\n",
    "Before we can start optimizing we need to convert our vanilla transformers model to the onnx format. The model we are using is the deepset/roberta-base-squad2 a fine-tuned RoBERTa model on the SQUAD2 question answering dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b64858a5-0e67-49db-9c5e-899c248e9467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verneylmavt/anaconda3/envs/ai/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The argument `from_transformers` is deprecated, and will be removed in optimum 2.0.  Use `export` instead\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.904166042804718, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "model_id = \"deepset/roberta-base-squad2\"\n",
    "onnx_path = Path(\"onnx\")\n",
    "task = \"question-answering\"\n",
    "\n",
    "# load vanilla transformers and convert to onnx\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(model_id, from_transformers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# save onnx checkpoint and tokenizer\n",
    "model.save_pretrained(onnx_path)\n",
    "tokenizer.save_pretrained(onnx_path)\n",
    "\n",
    "# test the model with using transformers pipeline, with handle_impossible_answer for squad_v2\n",
    "optimum_qa = pipeline(task, model=model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.9041663408279419, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b706078b-b646-4cdf-8677-1323545a0f2c",
   "metadata": {},
   "source": [
    "We successfully converted our vanilla transformers to onnx and used the model with the transformers.pipelines to run the first prediction. Now let's optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec3daaa-bb98-4917-9326-18ba2b10266d",
   "metadata": {},
   "source": [
    "## Use the ORTOptimizer to optimize the model\n",
    "After we saved our onnx checkpoint to onnx/ we can now use the ORTOptimizer to apply graph optimization, such as operator fusion and constant folding to accelerate latency and inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bafe50d-a582-4e25-8ff1-b80b32796274",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/verneylmavt/anaconda3/envs/ai/lib/python3.10/site-packages/optimum/onnxruntime/configuration.py:784: FutureWarning: disable_embed_layer_norm will be deprecated soon, use disable_embed_layer_norm_fusion instead, disable_embed_layer_norm_fusion is set to True.\n",
      "  warnings.warn(\n",
      "\u001b[0;93m2025-02-20 11:43:04.979606 [W:onnxruntime:, inference_session.cc:2048 Initialize] Serializing optimized model with Graph Optimization level greater than ORT_ENABLE_EXTENDED and the NchwcTransformer enabled. The generated model may contain hardware specific optimizations, and should only be used in the same environment the model was optimized in.\u001b[m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTOptimizer\n",
    "from optimum.onnxruntime.configuration import OptimizationConfig\n",
    "\n",
    "# create ORTOptimizer and define optimization configuration\n",
    "optimizer = ORTOptimizer.from_pretrained(onnx_path)\n",
    "\n",
    "optimization_config = OptimizationConfig(optimization_level=99) # enable all optimizations\n",
    "\n",
    "# apply the optimization configuration to the model\n",
    "optimizer.optimize(save_dir=onnx_path, optimization_config=optimization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdb2961-1b15-4356-aa80-a0eb63c249cc",
   "metadata": {},
   "source": [
    "To test performance we can use the ORTModelForQuestionAnswering class again and provide an additional file_name parameter to load our optimized model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fd1c42-7921-4b54-b037-1b9974230772",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.9041661620140076, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "# load optimized model\n",
    "opt_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_optimized.onnx\")\n",
    "\n",
    "# test the quantized model with using transformers pipeline\n",
    "opt_optimum_qa = pipeline(task, model=opt_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = opt_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "print(prediction)\n",
    "# {'score': 0.9041661620140076, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4fc696-72b2-45af-9b2f-310ca4d70e56",
   "metadata": {},
   "source": [
    "## Use the ORTQuantizer to apply dynamic quantization\n",
    "Another option to reduce model size and accelerate inference is by quantizing the model using the ORTQuantizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6470da15-abf3-4b45-9eee-65a8bc42de76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('onnx')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "\n",
    "# create ORTQuantizer and define quantization configuration\n",
    "quantizer = ORTQuantizer.from_pretrained(onnx_path, file_name=\"model.onnx\")\n",
    "qconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=True)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "quantizer.quantize(save_dir=onnx_path, quantization_config=qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784792ee-46d7-47f6-b995-3ce99fdb9d35",
   "metadata": {},
   "source": [
    "We can now compare this model size as well as some latency performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af92e2c-a508-4f56-9f2a-adb2a5419706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vanilla Onnx Model file size: 473.54 MB\n",
      "Quantized Onnx Model file size: 119.61 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# get model file size\n",
    "size = os.path.getsize(onnx_path / \"model.onnx\")/(1024*1024)\n",
    "print(f\"Vanilla Onnx Model file size: {size:.2f} MB\")\n",
    "size = os.path.getsize(onnx_path / \"model_quantized.onnx\")/(1024*1024)\n",
    "print(f\"Quantized Onnx Model file size: {size:.2f} MB\")\n",
    "\n",
    "# Vanilla Onnx Model file size: 473.51 MB\n",
    "# Quantized Onnx Model file size: 119.15 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c57cd0-2c1e-4972-8348-1a50cde5dea2",
   "metadata": {},
   "source": [
    "## Run accelerated inference using pipelines\n",
    "\n",
    "Optimum has built-in support for transformers pipelines. This allows us to leverage the same API that we know from using PyTorch and TensorFlow models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35cde70-69eb-418a-b3e7-e79d298c618c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8086288571357727, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, pipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "quant_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "\n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=quant_model, tokenizer=tokenizer)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.806605339050293, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ab4bb-bb57-490e-862a-727d9fbe4b4c",
   "metadata": {},
   "source": [
    "In addition to this optimum has a pipelines API which guarantees more safety for your accelerated models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "895cf453-8153-44ad-9fb2-3790482f6467",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 0.8086288571357727, 'start': 11, 'end': 18, 'answer': 'Philipp'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from optimum.pipelines import pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "quant_model = ORTModelForQuestionAnswering.from_pretrained(onnx_path, file_name=\"model_quantized.onnx\")\n",
    "                                                     \n",
    "quantized_optimum_qa = pipeline(\"question-answering\", model=quant_model, tokenizer=tokenizer, handle_impossible_answer=True)\n",
    "prediction = quantized_optimum_qa(question=\"What's my name?\", context=\"My name is Philipp and I live in Nuremberg.\")\n",
    "\n",
    "print(prediction)\n",
    "# {'score': 0.806605339050293, 'start': 11, 'end': 18, 'answer': 'Philipp'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcb03c5-74cd-42f7-bcfe-02b53c5a095d",
   "metadata": {},
   "source": [
    "## Evaluate performance and speed\n",
    "As the last step, we want to take a detailed look at the performance and accuracy of our model. Applying optimization techniques, like graph optimizations or quantization not only impact performance (latency) those also might have an impact on the accuracy of the model. So accelerating your model comes with a trade-off.\n",
    "\n",
    "Let's evaluate our models. Our transformers model deepset/roberta-base-squad2 was fine-tuned on the SQUAD2 dataset. This will be the dataset we use to evaluate our models.\n",
    "To safe time, we only load 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6508dc2c-9d64-4193-89c8-59aff4381329",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_metric' from 'datasets' (/Users/verneylmavt/anaconda3/envs/ai/lib/python3.10/site-packages/datasets/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_metric,load_dataset\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# load 10% of the data to safe time\u001b[39;00m\n\u001b[1;32m      4\u001b[0m metric \u001b[38;5;241m=\u001b[39m load_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquad_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_metric' from 'datasets' (/Users/verneylmavt/anaconda3/envs/ai/lib/python3.10/site-packages/datasets/__init__.py)"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric,load_dataset\n",
    "\n",
    "# load 10% of the data to safe time\n",
    "metric = load_metric(\"squad_v2\")\n",
    "dataset = load_dataset(\"squad_v2\", split=\"validation[:10%]\")\n",
    "\n",
    "print(f\"length of dataset {len(dataset)}\")\n",
    "#length of dataset 1187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4df13f1-b2ef-4db8-86cc-fe2fcc93c68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(example):\n",
    "  default = optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  optimized = opt_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  quantized = quantized_optimum_qa(question=example[\"question\"], context=example[\"context\"])\n",
    "  return {\n",
    "      'reference': {'id': example['id'], 'answers': example['answers']},\n",
    "      'default': {'id': example['id'],'prediction_text': default['answer'], 'no_answer_probability': 0.},\n",
    "      'optimized': {'id': example['id'],'prediction_text': optimized['answer'], 'no_answer_probability': 0.},\n",
    "      'quantized': {'id': example['id'],'prediction_text': quantized['answer'], 'no_answer_probability': 0.},\n",
    "      }\n",
    "\n",
    "result = dataset.map(evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea9364-d017-4cde-a394-6d6846c8e32e",
   "metadata": {},
   "source": [
    "Now lets compare the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22246ce-d719-4323-b5c8-5678064a1bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_acc = metric.compute(predictions=result[\"default\"], references=result[\"reference\"])\n",
    "optimized = metric.compute(predictions=result[\"optimized\"], references=result[\"reference\"])\n",
    "quantized = metric.compute(predictions=result[\"quantized\"], references=result[\"reference\"])\n",
    "\n",
    "print(f\"vanilla model: exact={default_acc['exact']}% f1={default_acc['f1']}%\")\n",
    "print(f\"optimized model: exact={quantized['exact']}% f1={optimized['f1']}%\")\n",
    "print(f\"quantized model: exact={quantized['exact']}% f1={quantized['f1']}%\")\n",
    "\n",
    "# vanilla model: exact=81.12889637742207% f1=83.27089343306695%\n",
    "# quantized model: exact=80.6234203875316% f1=82.86541222514259%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a2b7b-cc99-46b1-8c2a-56a804b2c37f",
   "metadata": {},
   "source": [
    "The quantized model achived an exact match of 80.62% and an f1 score of 82.86% which is 99% of the original model.\n",
    "\n",
    "Okay, let's test the performance (latency) of our optimized and quantized model.\n",
    "\n",
    "But first, letâ€™s extend our context and question to a more meaningful sequence length of 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f32a3-65c5-4e0b-b72f-fc1b52a8948a",
   "metadata": {},
   "outputs": [],
   "source": [
    "context=\"Hello, my name is Philipp and I live in Nuremberg, Germany. Currently I am working as a Technical Lead at Hugging Face to democratize artificial intelligence through open source and open science. In the past I designed and implemented cloud-native machine learning architectures for fin-tech and insurance companies. I found my passion for cloud concepts and machine learning 5 years ago. Since then I never stopped learning. Currently, I am focusing myself in the area NLP and how to leverage models like BERT, Roberta, T5, ViT, and GPT2 to generate business value.\"\n",
    "question=\"As what is Philipp working?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec406102-ba65-4d05-bb10-10703753be58",
   "metadata": {},
   "source": [
    "To keep it simple, we are going to use a python loop and calculate the avg/mean latency for our vanilla model and for the optimized and quantized model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef28ced-87a0-4096-9a43-2dcdf3f6a7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "import numpy as np\n",
    "\n",
    "def measure_latency(pipe):\n",
    "    latencies = []\n",
    "    # warm up\n",
    "    for _ in range(10):\n",
    "        _ = pipe(question=question, context=context)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ =  pipe(question=question, context=context)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    return f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\"\n",
    "\n",
    "print(f\"Vanilla model {measure_latency(optimum_qa)}\")\n",
    "print(f\"Optimized model {measure_latency(opt_optimum_qa)}\")\n",
    "print(f\"Quantized model {measure_latency(quantized_optimum_qa)}\")\n",
    "\n",
    "# Vanilla model Average latency (ms) 102\n",
    "# Optimized model Average latency (ms) 101\n",
    "# Quantized model Average latency (ms) 46"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245313e-f7ea-427b-8fd5-d7ea3efdab68",
   "metadata": {},
   "source": [
    "We managed to reduce our model latency from 102ms to 46ms or by 55%, while keeping 99% of the accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
